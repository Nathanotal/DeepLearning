{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def LoadBatch(file): \n",
    "    \n",
    "    with open(\"data/\"+file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    \n",
    "    pixelDat = dict[b'data']\n",
    "    labels = dict[b'labels']\n",
    "    labelsOneHot = np.zeros((len(labels),10))\n",
    "    \n",
    "    for index in range(len(labels)): # Not efficient :)\n",
    "        labelsOneHot[index][labels[index]] = 1\n",
    "        \n",
    "    return pixelDat, labelsOneHot, labels\n",
    "\n",
    "def CalcS(X, W, b):\n",
    "    return W @ X + b\n",
    "\n",
    "def getXk(X, W, b): # For the intermediary steps\n",
    "    return np.maximum(0, CalcS(X, W, b))\n",
    "\n",
    "def getP(X, W, b): # For the final step\n",
    "    return softmax(CalcS(X, W, b))\n",
    "\n",
    "def batchNormalize(S, mean, variance): # 11\n",
    "    t1 = (S - mean[:, np.newaxis])\n",
    "    t2 = np.diag(np.sqrt(variance + np.finfo(float).eps))\n",
    "    t2inv = np.linalg.inv(t2)\n",
    "    S_hat = np.dot(t2inv, t1)\n",
    "    return S_hat\n",
    "\n",
    "def forwardPass(X_batch, WList, bList, gammaList=None, betaList=None, mean=None, var=None, doBatchNormalization=False):\n",
    "    Xk = X_batch\n",
    "    XList = [Xk] # Get history for gradient calculations\n",
    "    S_List, S_hatList, meanList, varList = [], [], [], []\n",
    "    WLast = WList.pop() # Remove the last weight matrix\n",
    "    bLast = bList.pop()\n",
    "\n",
    "    for index, [W, b] in enumerate(zip(WList, bList)): # Do all intermediary calculations\n",
    "        if doBatchNormalization: # Get mean and variance for batch normalization\n",
    "            S = CalcS(Xk, W, b)\n",
    "            S_List.append(S)\n",
    "            mean, variance = np.mean(S, axis=1), np.var(S, axis=1)\n",
    "            S_hat = batchNormalize(S, mean, variance) \n",
    "            S_tilda = np.multiply(S_hat, gammaList[index]) + betaList[index] # Apply shift and scale, does not seem to have to be implemented\n",
    "            Xk = np.maximum(0, S_tilda)\n",
    "            S_hatList.append(S_hat), meanList.append(mean), varList.append(variance)\n",
    "        else:\n",
    "            Xk = getXk(Xk, W, b)\n",
    "        XList.append(Xk)\n",
    "    S = CalcS(Xk, WLast, bLast)\n",
    "    S_List.append(S)\n",
    "    P = softmax(S) # Get the final probability\n",
    "    return P, XList, S_hatList, meanList, varList, S_List\n",
    "\n",
    "def batchNormBackPass(G_vec, S_Batch, mean, var):\n",
    "    sig1 = np.power(var + np.finfo(float).eps, -0.5).T # 31\n",
    "    sig2 = np.power(var + np.finfo(float).eps, -1.5).T #32\n",
    "    G1 = np.multiply(G_vec, sig1[:, np.newaxis]) # 33\n",
    "    G2 = np.multiply(G_vec, sig2[:, np.newaxis]) # 34\n",
    "    D = S_Batch - mean[:, np.newaxis] # 35\n",
    "    c = np.sum(np.multiply(G2, D), axis=1)[:, np.newaxis] # 36\n",
    "    denom = len(S_Batch[0])\n",
    "    t1 = np.sum(G1, axis=1)[:, np.newaxis] # 37a\n",
    "    t2 = (np.multiply(D, c)) # 37b\n",
    "    G_vec = G1 - (t1 / denom) - (t2 / denom) # 37c\n",
    "    return G_vec\n",
    "\n",
    "def backwardPass(P_batch, X_batch, XList, Y_batch, WList, lamb, n_batch, S_List, S_hatList=None, gammaList=None, meanList=None, varList=None, doBatchNormalization=False):\n",
    "    if doBatchNormalization:\n",
    "        # Walk backwards through the intermediary calculations\n",
    "        XList.reverse(), WList.reverse(),gammaList.reverse(),varList.reverse(),meanList.reverse(), S_hatList.reverse(),S_List.reverse()\n",
    "        W_last = WList.pop() # Get the last weight matrix for the initial computation\n",
    "        G_vec = - (Y_batch-P_batch) # Initialize G_vec\n",
    "        dJdW_list, dJdb_list, dJdgamma_list, dJdbeta_list = [], [], [], []\n",
    "        a = 0 # Debugging\n",
    "        for index, [Xk, W] in enumerate(zip(XList, WList)): # Loop through all intermediary steps\n",
    "            if index == 0: # First computation, last gradient\n",
    "                a += 1\n",
    "                Xk_batch = Xk\n",
    "                dJdW = ((G_vec @ Xk_batch.T))/n_batch\n",
    "                dJdb = np.sum(G_vec, axis=1)[:, np.newaxis]/n_batch\n",
    "                dJdW_list.append(dJdW + 2*lamb*W), dJdb_list.append(dJdb)\n",
    "                G_vec = W.T @ G_vec # Update G_vec for the next computation\n",
    "                Xk_batch[Xk_batch<0] = 0\n",
    "                Xk_batch[Xk_batch>0] = 1\n",
    "                G_vec = np.multiply(G_vec, Xk_batch)\n",
    "            else: # Compute the gradient with respect to the batch normalization parameters\n",
    "                Xk_batch = Xk\n",
    "                dJdgamma = np.sum(np.multiply(G_vec, S_hatList[index-1]), axis=1)[:, np.newaxis]/n_batch # 25a\n",
    "                dJdbeta = np.sum(G_vec, axis=1)[:, np.newaxis]/n_batch # 25b\n",
    "                G_vec = G_vec * gammaList[index-1] # 26\n",
    "                G_vec = batchNormBackPass(G_vec, S_List[index], meanList[index-1], varList[index-1]) # 27\n",
    "                dJdW = ((G_vec @ Xk.T))/n_batch + 2*lamb*W\n",
    "                dJdb = np.sum(G_vec, axis=1)[:, np.newaxis]/n_batch\n",
    "                dJdW_list.append(dJdW), dJdb_list.append(dJdb), dJdgamma_list.append(dJdgamma), dJdbeta_list.append(dJdbeta)\n",
    "                a += 1\n",
    "                \n",
    "                G_vec = W.T @ G_vec # Update G_vec for the next computation\n",
    "                Xk_batch[Xk_batch<0] = 0\n",
    "                Xk_batch[Xk_batch>0] = 1\n",
    "                G_vec = np.multiply(G_vec, Xk_batch)\n",
    "\n",
    "        # Get the final gradient from the input X, (first gradient)\n",
    "        dJdgamma = np.sum(np.multiply(G_vec, S_hatList[-1]), axis=1)[:, np.newaxis]/n_batch # 25a\n",
    "        dJdbeta = np.sum(G_vec, axis=1)[:, np.newaxis]/n_batch # 25b\n",
    "        G_vec = G_vec * gammaList[-1] # 26\n",
    "        G_vec = batchNormBackPass(G_vec, S_List[-1], meanList[-1], varList[-1]) # 27\n",
    "        dJdW = ((G_vec @ X_batch.T))/n_batch + 2*lamb*W_last\n",
    "        dJdb = np.sum(G_vec, axis=1)[:, np.newaxis]/n_batch\n",
    "        dJdW_list.append(dJdW), dJdb_list.append(dJdb), dJdgamma_list.append(dJdgamma), dJdbeta_list.append(dJdbeta)\n",
    "        \n",
    "        a += 1\n",
    "        dJdW_list.reverse(), dJdb_list.reverse(), dJdgamma_list.reverse(), dJdbeta_list.reverse()\n",
    "\n",
    "        return dJdW_list, dJdb_list, dJdgamma_list, dJdbeta_list\n",
    "    else:\n",
    "        XList.reverse() # Walk backwards through the intermediary calculations\n",
    "        WList.reverse()\n",
    "        W_last = WList.pop() # Get the last weight matrix for the initial computation\n",
    "        G_vec = - (Y_batch-P_batch) # Initialize G_vec\n",
    "        dJdW_list = []\n",
    "        dJdb_list = []\n",
    "        for Xk, W in zip(XList, WList): # Loop through all intermediary steps\n",
    "            Xk_batch = Xk\n",
    "            dJdW = ((G_vec @ Xk_batch.T))/n_batch\n",
    "            dJdb = np.sum(G_vec, axis=1)[:, np.newaxis]/n_batch\n",
    "            dJdW_list.append(dJdW + 2*lamb*W)\n",
    "            dJdb_list.append(dJdb)\n",
    "            \n",
    "            G_vec = W.T @ G_vec # Update G_vec for the next computation\n",
    "            Xk_batch[Xk_batch<0] = 0\n",
    "            Xk_batch[Xk_batch>0] = 1\n",
    "            G_vec = np.multiply(G_vec, Xk_batch)\n",
    "        \n",
    "        # Get the final gradient from the input X\n",
    "        dJdW = ((G_vec @ X_batch.T))/n_batch\n",
    "        dJdb = np.sum(G_vec, axis=1)[:, np.newaxis]/n_batch\n",
    "        dJdW_list.append(dJdW + 2*lamb*W_last)\n",
    "        dJdb_list.append(dJdb)\n",
    "        dJdW_list.reverse() \n",
    "        dJdb_list.reverse()\n",
    "        \n",
    "        return dJdW_list, dJdb_list, None, None\n",
    "\n",
    "def ComputeCost(X, Y, W_list, b_list, lamb, gammaList, betaList, movingMeanList=[], movingVarList=[], doBatchNormalization=False): \n",
    "    P,_,_,_,_,_ = forwardPass(X, W_list.copy(), b_list.copy(), gammaList.copy(), betaList.copy(), movingMeanList.copy(), movingVarList.copy(), doBatchNormalization)\n",
    "    J = 0\n",
    "    P_t = np.clip(P.T, 1e-15, None)\n",
    "    for i in range(len(P_t)):\n",
    "        J += -np.dot(Y[i], np.log(P_t[i], where=P_t[i] > 0))\n",
    "    J /= len(X[0]) # Divide by dimensionality\n",
    "    loss = J # For documentation\n",
    "    J += lamb * (np.sum([np.sum(np.power(W,2)) for W in W_list])) # Regularization\n",
    "    \n",
    "    return J, loss\n",
    "\n",
    "def ComputeAccuracy(X, y, W_list, b_list, gammaList, betaList, movingMeanList=[], movingVarList=[], doBatchNormalization=True): \n",
    "    P,_,_,_,_,_ = forwardPass(X, W_list.copy(), b_list.copy(), gammaList.copy(), betaList.copy(), movingMeanList.copy(), movingVarList.copy(), doBatchNormalization)\n",
    "    nCorr = 0\n",
    "    for index in range(X.T.shape[0]):\n",
    "        p = P.T[index]\n",
    "        predClass = np.argmax(p)\n",
    "        if predClass == y[index]:\n",
    "            nCorr += 1\n",
    "    \n",
    "    acc = nCorr/X.T.shape[0]\n",
    "    return acc\n",
    "\n",
    "def updateWeights(Var_list, dJdVar_list, lr):\n",
    "    Var_list = [Var - lr*dJdVar for Var, dJdVar in zip(Var_list, dJdVar_list)]\n",
    "    return Var_list\n",
    "\n",
    "def init_variables2(): # More training data\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = None, None, None, None, None, None\n",
    "    y_train = None\n",
    "    for file in [\"data_batch_1\", \"data_batch_2\", \"data_batch_3\", \"data_batch_4\", \"data_batch_5\", \"test_batch\"]:\n",
    "        X, Y, y = LoadBatch(file)\n",
    "        mean_X = np.mean(X, axis=0) \n",
    "        std_X = np.std(X, axis=0)\n",
    "        X = X - mean_X\n",
    "        X = X / std_X\n",
    "        X = X.T # Make x stored in columns\n",
    "        if file in [\"data_batch_1\",\"data_batch_3\", \"data_batch_4\", \"data_batch_5\"]:\n",
    "            if X_train is None:\n",
    "                X_train = X\n",
    "                Y_train = Y\n",
    "                y_train = y\n",
    "            else:\n",
    "                X_train = np.concatenate((X_train, X), axis=1)\n",
    "                Y_train = np.concatenate((Y_train, Y), axis=0)\n",
    "                y_train += y\n",
    "            \n",
    "        elif file == \"data_batch_2\":\n",
    "            X_val,Y_val, y_val = X.T[0:5000].T, Y[0:5000], y[0:5000]\n",
    "            X_train = np.concatenate((X_train, X.T[5000:].T), axis=1)\n",
    "            Y_train = np.concatenate((Y_train, Y[5000:]), axis=0)\n",
    "            y_train += y[5000:]\n",
    "        else:\n",
    "            X_test,Y_test, y_test = X, Y, y\n",
    "           \n",
    "    return X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test\n",
    "\n",
    "def init_variables(intermediary_layer_sizes=[50, 50], all=False): # Excercise 1, [50, 30, 20, 20, 10, 10, 10, 10]\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = None, None, None, None, None, None\n",
    "    if all:\n",
    "        X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test = init_variables2()\n",
    "    else:\n",
    "        for file in [\"data_batch_1\", \"data_batch_2\", \"test_batch\"]:\n",
    "            X, Y, y = LoadBatch(file)\n",
    "            mean_X = np.mean(X, axis=0) \n",
    "            std_X = np.std(X, axis=0)\n",
    "            X = X - mean_X\n",
    "            X = X / std_X\n",
    "            X = X.T # Make x stored in columns\n",
    "            if file == \"data_batch_1\":\n",
    "                X_train,Y_train, y_train = X, Y, y\n",
    "            elif file == \"data_batch_2\":\n",
    "                X_val,Y_val, y_val = X, Y, y\n",
    "            else:\n",
    "                X_test,Y_test, y_test = X, Y, y\n",
    "    \n",
    "    initExperiment = False\n",
    "    sigma = 0.0001 # 0.1, 0.001, 0.0001\n",
    "    \n",
    "    np.random.seed(111)\n",
    "    eta_min = 0.00001\n",
    "    eta_max = 0.1\n",
    "    K = 10 # Number of labels\n",
    "    d = len(X_train.T[0]) # dimensionality\n",
    "    n = 0\n",
    "    W_list = []\n",
    "    b_list = []\n",
    "    prevSize = d\n",
    "    layer_sizes = intermediary_layer_sizes + [K]\n",
    "    gammaList = []\n",
    "    betaList = []\n",
    "    for size in layer_sizes: # Initialize weights and biases for all layers\n",
    "        if n == len(layer_sizes)-1: # If last layer\n",
    "            if initExperiment:\n",
    "                staticNormalizedW = np.random.normal(0, sigma, (K, prevSize))\n",
    "                W_list.append(staticNormalizedW)\n",
    "                print('experiment')\n",
    "            else:\n",
    "                W_list.append(np.random.normal(0, 2/np.sqrt(prevSize), (K, prevSize)))\n",
    "                print('standard')\n",
    "            b_list.append(np.zeros((K,1)))\n",
    "        else:\n",
    "            if initExperiment:\n",
    "                staticNormalizedW = np.random.normal(0, sigma, (size, prevSize))\n",
    "                W_list.append(staticNormalizedW)\n",
    "                print('experiment')\n",
    "            else:\n",
    "                W_list.append(np.random.normal(0, 2/np.sqrt(prevSize), (size, prevSize)))\n",
    "                print('standard')\n",
    "            b_list.append(np.zeros((size,1)))\n",
    "            gammaList.append(np.ones((size,1)))\n",
    "            betaList.append(np.zeros((size,1)))\n",
    "            prevSize = size\n",
    "            \n",
    "        # W1 = np.random.normal(0, 1/np.sqrt(d), (m, d)) \n",
    "        # W2 = np.random.normal(0, 1/np.sqrt(m), (K, m)) \n",
    "        # b1 = np.zeros((m,1)) # np.random.normal(0, 0.01, (m,1))\n",
    "        # b2 = np.zeros((K,1)) # np.random.normal(0, 0.01, (K,1))\n",
    "        n += 1\n",
    "    n_s=800 \n",
    "    return X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, W_list, b_list, eta_min, eta_max, n_s, gammaList, betaList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, W_list, b_list, eta_min, eta_max,_, gammaList, betaList = init_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lamb = 0\n",
    "# P, X_List, S_hatList, meanList, varList, S_List = forwardPass(X_train, W_list.copy(), gammaList.copy(), betaList.copy(), b_list.copy(), doBatchNormalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dJdW_list, dJdb_list, dJdgamma_list, dJdbeta_list = backwardPass(P, X_batch, X_List.copy(), Y_batch,W_list.copy(), 0.001, 100, S_List.copy(), S_hatList.copy(),gammaList.copy(), meanList.copy(), varList.copy(), doBatchNormalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import grads_check\n",
    "# J,_,_ = ComputeCost(X_train, Y_train, W_list, b_list, lamb, gammaList, betaList, None, None, True)\n",
    "# acc = ComputeAccuracy(X_train, y_train, W1, W2, b1, b2)\n",
    "# grad_W1, grad_W2, grad_b1, grad_b2 = grads_check.ComputeGradients(X_train, Y_train, P, H, W1, W2, lamb, b_start=0, b_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrhist = []\n",
    "def getLr(t, eta_min, eta_max, n_s):\n",
    "    if (int(t/n_s))%2 == 0:\n",
    "        return eta_min + (eta_max-eta_min)*((t%n_s)/n_s)\n",
    "    else:\n",
    "        return eta_max - (eta_max-eta_min)*((t%n_s)/n_s)\n",
    "\n",
    "def suffleDatapoints(X, Y,y):\n",
    "    p = np.random.permutation(len(Y))\n",
    "    return X.T[p].T, Y[p], np.asarray(y)[p]\n",
    "  \n",
    "def MiniBatchGD(X, Y, y, W_list, b_list, lamb, n_epochs, n_batch, eta_min, eta_max, X_val, Y_val, y_val, n_s, gammaList, betaList):         \n",
    "    acc_hist,cost_hist, loss_hist, acc_hist_val, cost_hist_val, loss_hist_val,loss_hist_val = [] ,[], [], [], [], [], []\n",
    "    # Train, initial val\n",
    "    acc = ComputeAccuracy(X, y, W_list, b_list, gammaList, betaList)\n",
    "    cost, loss = ComputeCost(X, Y, W_list, b_list, lamb, gammaList, betaList)\n",
    "    acc_hist.append(acc), cost_hist.append(cost), loss_hist.append(loss)\n",
    "    # Validation, initial val\n",
    "    acc = ComputeAccuracy(X_val, y_val, W_list, b_list, gammaList, betaList)\n",
    "    cost, loss = ComputeCost(X_val, Y_val, W_list, b_list, lamb, gammaList, betaList)\n",
    "    acc_hist_val.append(acc), cost_hist_val.append(cost), loss_hist_val.append(loss)\n",
    "    t = 0\n",
    "    doBatchNormalization = True\n",
    "    movingMeanList = []\n",
    "    movingVarList = []\n",
    "    lr = eta_min\n",
    "    total_batches = int(len(Y)/n_batch)\n",
    "    for epoch in range(n_epochs): # Main loop\n",
    "        X, Y, y = suffleDatapoints(X, Y, y)\n",
    "        for batch in range(total_batches):\n",
    "            t+=1\n",
    "            lrhist.append(lr)\n",
    "            X_batch = X.T[batch*n_batch:batch*n_batch+n_batch].T\n",
    "            Y_batch = Y[batch*n_batch:batch*n_batch+n_batch].T\n",
    "            \n",
    "            P_Batch, X_List, S_hatList, meanList, varList, S_List = forwardPass(X_batch, W_list.copy(), b_list.copy(), gammaList.copy(), betaList.copy(), None, None, doBatchNormalization)\n",
    "\n",
    "            dJdW_list, dJdb_list, dJdgamma_list, dJdbeta_list = backwardPass(P_Batch, X_batch, X_List.copy(), Y_batch,W_list.copy(), lamb, n_batch, S_List.copy(), S_hatList.copy(),gammaList.copy(), meanList.copy(), varList.copy(), doBatchNormalization)\n",
    "            \n",
    "            W_list = updateWeights(W_list, dJdW_list, lr)\n",
    "            b_list = updateWeights(b_list, dJdb_list, lr)\n",
    "            if doBatchNormalization:\n",
    "                gammaList = updateWeights(gammaList, dJdgamma_list, lr)\n",
    "                betaList = updateWeights(betaList, dJdbeta_list, lr)\n",
    "            \n",
    "            # Do exponential moving average for the mean and variance\n",
    "            if batch == 0:\n",
    "                movingMeanList = meanList\n",
    "                movingVarList = varList\n",
    "            else:\n",
    "                movingMeanList = [0.9*mm + 0.1*m for mm, m in zip(movingMeanList, meanList)]\n",
    "                movingVarList = [0.9*mv + 0.1*v for mv, v in zip(movingVarList, varList)]\n",
    "            lr = getLr(t, eta_min, eta_max, n_s)\n",
    "            \n",
    "        # Train\n",
    "        acc = ComputeAccuracy(X, y, W_list, b_list, gammaList, betaList, movingMeanList, movingVarList, doBatchNormalization)\n",
    "        cost, loss = ComputeCost(X, Y, W_list, b_list, lamb, gammaList, betaList, movingMeanList, movingVarList, doBatchNormalization)\n",
    "        acc_hist.append(acc), cost_hist.append(cost), loss_hist.append(loss)\n",
    "        # Validation\n",
    "        acc = ComputeAccuracy(X_val, y_val, W_list, b_list, gammaList, betaList, movingMeanList, movingVarList, doBatchNormalization)\n",
    "        cost, loss = ComputeCost(X_val, Y_val, W_list, b_list, lamb, gammaList, betaList, movingMeanList, movingVarList, doBatchNormalization)\n",
    "        acc_hist_val.append(acc), cost_hist_val.append(cost), loss_hist_val.append(loss)\n",
    "        print(\"Epoch:\", epoch, \": Train accuracy:\", acc_hist[-1], \", Val accuracy:\", acc_hist_val[-1])\n",
    "        \n",
    "    return W_list, b_list, cost_hist, acc_hist, loss_hist, cost_hist_val, acc_hist_val, loss_hist_val, movingMeanList, movingVarList, gammaList, betaList\n",
    "\n",
    "\n",
    "\n",
    "Whist, bhist, cost_hist_list, acc_hist_list, loss_hist_list, cost_hist_list_val, acc_hist_list_val, loss_hist_list_val = [], [], [], [], [], [], [], []\n",
    "gammaListHist, betaListHist, movingMeanListHist, movingVarListHist = [], [], [], []\n",
    "X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, W_list, b_list, eta_min, eta_max, n_s, gammaList, betaList = init_variables(all=True) \n",
    "\n",
    "n_epochs = 30 # 3 cycles\n",
    "n_batch = 100\n",
    "n_s = 5 * int(len(X_train[0])) / n_batch\n",
    "lamb = 0.005\n",
    "W_list, b_list, cost_hist, acc_hist, loss_hist, cost_hist_val, acc_hist_val, loss_hist_val, movingMeanList, movingVarList, gammaList, betaList = MiniBatchGD(X=X_train, Y=Y_train, y=y_train, W_list=W_list, b_list=b_list, lamb=lamb, n_epochs=n_epochs, n_batch=n_batch, eta_min=eta_min, eta_max=eta_max, X_val=X_val, Y_val=Y_val, y_val=y_val, n_s=n_s, gammaList=gammaList, betaList=betaList)\n",
    "Whist.append(W_list), bhist.append(b_list), cost_hist_list.append(cost_hist), acc_hist_list.append(acc_hist), loss_hist_list.append(loss_hist), cost_hist_list_val.append(cost_hist_val), acc_hist_list_val.append(acc_hist_val), loss_hist_list_val.append(loss_hist_val)\n",
    "gammaListHist.append(gammaList), betaListHist.append(betaList), movingMeanListHist.append(movingMeanList), movingVarListHist.append(movingVarList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for a good lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaSearch():\n",
    "    Whist, bhist, cost_hist_list, acc_hist_list, loss_hist_list, cost_hist_list_val, acc_hist_list_val, loss_hist_list_val = [], [], [], [], [], [], [], []\n",
    "    X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, W_list, b_list, eta_min, eta_max, n_s, gammaList, betaList = init_variables(all=True) \n",
    "    gammaListHist, betaListHist, movingMeanListHist, movingVarListHist = [], [], [], []\n",
    "    \n",
    "    \n",
    "    lamb = 0.01\n",
    "    n_epochs = 30 # 1.5 \n",
    "    n_batch = 100\n",
    "    n_s = 5 * int(len(X_train[0])) / n_batch\n",
    "    lamb = 0.0045\n",
    "    lambdas = [0.00475] # , 0.01, 0.005, 0.001, 0.0005, 0.0001. ####### 0.01 - 0.001: 0.0025, 0.0075, 0.00375, 0.00675, 0.00425, 0.00575\n",
    "    for lamb in lambdas:\n",
    "        W_list, b_list, cost_hist, acc_hist, loss_hist, cost_hist_val, acc_hist_val, loss_hist_val, movingMeanList, movingVarList, gammaList, betaList = MiniBatchGD(X=X_train.copy(), Y=Y_train.copy(), y=y_train.copy(), W_list=W_list.copy(), b_list=b_list.copy(), lamb=lamb, n_epochs=n_epochs, n_batch=n_batch, eta_min=eta_min, eta_max=eta_max, X_val=X_val.copy(), Y_val=Y_val.copy(), y_val=y_val.copy(), n_s=n_s, gammaList=gammaList.copy(), betaList=betaList.copy())\n",
    "        Whist.append(W_list), bhist.append(b_list), cost_hist_list.append(cost_hist), acc_hist_list.append(acc_hist), loss_hist_list.append(loss_hist), cost_hist_list_val.append(cost_hist_val), acc_hist_list_val.append(acc_hist_val), loss_hist_list_val.append(loss_hist_val)\n",
    "        gammaListHist.append(gammaList), betaListHist.append(betaList), movingMeanListHist.append(movingMeanList), movingVarListHist.append(movingVarList)\n",
    "    \n",
    "    x = [i for i in range(n_epochs+1)]\n",
    "    for index, [lamb, accval, acctrain] in enumerate(zip(lambdas, acc_hist_list_val, acc_hist_list)):\n",
    "        plt.clf()\n",
    "        plt.title(\"Accuracy graph, lambda = \" + str(lamb))\n",
    "        plt.plot(x, acctrain, label = \"Training\")\n",
    "        plt.plot(x, accval, label = \"Valuation\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        print(\"Final test accuracy (lamda = \"+str(lamb)+\"):\", ComputeAccuracy(X_test, y_test, Whist[index], bhist[index], gammaListHist[index], betaListHist[index], movingMeanListHist[index], movingVarListHist[index], True))\n",
    "    return X_test, y_test, Whist, bhist, movingMeanList, movingVarList, gammaList, betaList\n",
    "        \n",
    "# X_test, y_test, Whist, bhist, movingMeanList, movingVarList, gammaList, betaList = lambdaSearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(len(lrhist))]\n",
    "plt.clf()\n",
    "plt.title(\"Learning rate\")\n",
    "plt.plot(x, lrhist, label = \"Lr\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "x = [i for i in range(n_epochs+1)]\n",
    "\n",
    "for i in range(len(Whist)):\n",
    "    Whist_t, bhist_t, cost_hist_t, acc_hist_t, loss_hist_t, cost_hist_val_t, acc_hist_val_t, loss_hist_val_t = Whist[i], bhist[i], cost_hist_list[i], acc_hist_list[i], loss_hist_list[i], cost_hist_list_val[i], acc_hist_list_val[i], loss_hist_list_val[i]\n",
    "    gammaListHist_t, betaListHist_t, movingMeanListHist_t, movingVarListHist_t = gammaListHist[i], betaListHist[i], movingMeanListHist[i], movingVarListHist[i]\n",
    "    print(\"\\nLambda:\", lamb, \"\\n----------------------------------\")\n",
    "    plt.clf()\n",
    "    plt.title(\"Cost graph\")\n",
    "    plt.plot(x, cost_hist_t, label = \"Training\")\n",
    "    plt.plot(x, cost_hist_val_t, label = \"Valuation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.title(\"Loss graph\")\n",
    "    plt.plot(x, loss_hist_t, label = \"Training\")\n",
    "    plt.plot(x, loss_hist_val_t, label = \"Valuation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.title(\"Accuracy graph\")\n",
    "    plt.plot(x, acc_hist_t, label = \"Training\")\n",
    "    plt.plot(x, acc_hist_val_t, label = \"Valuation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"Final test accuracy:\", ComputeAccuracy(X_test, y_test, Whist_t, bhist_t, gammaListHist_t, betaListHist_t, movingMeanListHist_t, movingVarListHist_t, True))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29d9c81cb61075413b5d4bcd9e758d2b8dbd2078305b37b0a3605925d8fe2bf1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
