{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def LoadBatch(file): # Excercise 1.1\n",
    "    \n",
    "    with open(\"data/\"+file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    \n",
    "    pixelDat = dict[b'data']\n",
    "    labels = dict[b'labels']\n",
    "    labelsOneHot = np.zeros((len(labels),10))\n",
    "    \n",
    "    for index in range(len(labels)): # Not efficient :)\n",
    "        labelsOneHot[index][labels[index]] = 1\n",
    "        \n",
    "    return pixelDat, labelsOneHot, labels\n",
    "\n",
    "def EvaluateClassifier(X, W, b): # Excercise 1.4, no need to transpose x!\n",
    "    s = W @ X + b # @ is matrix multiplication\n",
    "    return softmax(s)\n",
    "\n",
    "def ComputeCost(X, Y, W, b, lamb): # Excercise 1.5\n",
    "    J = 0\n",
    "    P = EvaluateClassifier(X, W, b)\n",
    "    P_t = P.T\n",
    "    for i in range(len(P_t)):\n",
    "        J += -np.dot(Y[i], np.log(P_t[i]))\n",
    "    J /= len(X[0]) # Divide by dimensionality\n",
    "    loss = J # For documentation\n",
    "    J += lamb * np.sum(np.power(W,2)) # WTerm\n",
    "    \n",
    "    return J, P, loss\n",
    "\n",
    "def ComputeAccuracy(X, y, W, b): # Excercise 1.6\n",
    "    nCorr = 0\n",
    "    P = EvaluateClassifier(X, W, b)\n",
    "    for index in range(X.T.shape[0]):\n",
    "        p = P.T[index]\n",
    "        predClass = np.argmax(p)\n",
    "        if predClass == y[index]:\n",
    "            nCorr += 1\n",
    "    \n",
    "    acc = nCorr/X.T.shape[0]\n",
    "    return acc\n",
    "\n",
    "def ComputeGradients(X, Y, P, W, lamb, b_start=0, b_size=None): # Excercise 1.7\n",
    "    if b_size is None:\n",
    "        b_size = len(Y)\n",
    "    # Get random subset of X and Y (not implemented as not required)\n",
    "    X_batch = X.T[b_start:b_start+b_size].T # Because Python\n",
    "    Y_batch = Y[b_start:b_start+b_size].T\n",
    "    P_batch = P.T[b_start:b_start+b_size].T\n",
    "    G_vec = - (Y_batch-P_batch) # G_vec is nxK\n",
    "    dldw = np.dot(G_vec, X_batch.T)/b_size # dldw is KxD\n",
    "    dldb = np.sum(G_vec, axis=1)/b_size # dldb is Kx1\n",
    "    grad_W = dldw + 2*lamb*W # gradW is KxD\n",
    "    grad_b = dldb # gradB is Kx1\n",
    "    return grad_W, grad_b\n",
    "\n",
    "def init_variables():\n",
    "    # Excercise 1.2\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = None, None, None, None, None, None\n",
    "    for file in [\"data_batch_1\", \"data_batch_2\", \"test_batch\"]:\n",
    "        X, Y, y = LoadBatch(file)\n",
    "        mean_X = np.mean(X, axis=0)\n",
    "        std_X = np.std(X, axis=0)\n",
    "        X = X - mean_X\n",
    "        X = X / std_X\n",
    "        X = X.T # Make x stored in columns\n",
    "        if file == \"data_batch_1\":\n",
    "            X_train,Y_train, y_train = X, Y, y\n",
    "        elif file == \"data_batch_2\":\n",
    "            X_val,Y_val, y_val = X, Y, y\n",
    "        else:\n",
    "            X_test,Y_test, y_test = X, Y, y\n",
    "    \n",
    "    # Excercise 1.3\n",
    "    np.random.seed(111)\n",
    "    K = 10 # Number of labels\n",
    "    d = len(X.T[0]) # dimensionality\n",
    "    W = np.random.normal(0, 0.01, (K, d)) # Wierd, check here\n",
    "    b = np.random.normal(0, 0.01, (K,1))\n",
    "            \n",
    "    return X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients():\n",
    "    X, Y, y, _, _, _, _, _, _, W, b = init_variables()\n",
    "    lamb = 0\n",
    "    P = EvaluateClassifier(X, W, b)\n",
    "    J,P,_ = ComputeCost(X, Y, W, b, lamb) # P is now nxK for easier handling\n",
    "    acc = ComputeAccuracy(X, y, W, b)\n",
    "    c, d = ComputeGradsNumSlow(X.T[0:20].T, Y[0:20], P.T[0:20].T, W, b, lamb, 10**-8)\n",
    "    a, b = ComputeGradients(X, Y, P, W, lamb,20)\n",
    "    print(\"Gradient check:\")\n",
    "    print(\"W:\")\n",
    "    print(\"Derived:\", a)\n",
    "    print(\"Check:\", c)\n",
    "    print(\"b:\")\n",
    "    print(\"Derived:\", b)\n",
    "    print(\"Check:\", d)\n",
    "    maxDiff = 0\n",
    "    for val in enumerate(np.nditer(a-c)):\n",
    "        diff = np.abs(val[1])\n",
    "        if diff > maxDiff:\n",
    "            maxDiff = diff\n",
    "    print(\"Max gradient difference:\", maxDiff)\n",
    "    maxDiff = 0\n",
    "    for val in enumerate(np.nditer(b-d)):\n",
    "        diff = np.abs(val[1])\n",
    "        if diff > maxDiff:\n",
    "            maxDiff = diff\n",
    "    print(\"Max gradient difference:\", maxDiff)\n",
    "\n",
    "if True:\n",
    "    check_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniBatchGD(X, Y, y, W, b, lamb, n_epochs, n_batch, eta, X_val, Y_val, y_val): # Excercise 1.8\n",
    "    acc_hist,cost_hist, loss_hist, acc_hist_val, cost_hist_val, loss_hist_val,loss_hist_val = [] ,[], [], [], [], [], []\n",
    "    # Train, initial val\n",
    "    acc = ComputeAccuracy(X, y, W, b)\n",
    "    cost, _, loss = ComputeCost(X, Y, W, b, lamb)\n",
    "    acc_hist.append(acc), cost_hist.append(cost), loss_hist.append(loss)\n",
    "    # Validation, initial val\n",
    "    acc = ComputeAccuracy(X_val, y_val, W, b)\n",
    "    cost, _, loss = ComputeCost(X_val, Y_val, W, b, lamb)\n",
    "    acc_hist_val.append(acc), cost_hist_val.append(cost), loss_hist_val.append(loss)\n",
    "    \n",
    "    for epoch in range(n_epochs): # Main loop\n",
    "        for batch in range(int(len(Y)/n_batch)):\n",
    "            P = EvaluateClassifier(X, W, b)\n",
    "            grad_W, grad_b = ComputeGradients(X, Y, P, W, lamb, b_start=batch*n_batch, b_size=n_batch)\n",
    "            W = W - grad_W*eta\n",
    "            grad_b = grad_b.reshape(b.shape)\n",
    "            b = b - grad_b*eta\n",
    "        # Train\n",
    "        acc = ComputeAccuracy(X, y, W, b)\n",
    "        cost, _, loss = ComputeCost(X, Y, W, b, lamb)\n",
    "        acc_hist.append(acc), cost_hist.append(cost), loss_hist.append(loss)\n",
    "        # Validation\n",
    "        acc = ComputeAccuracy(X_val, y_val, W, b)\n",
    "        cost, _, loss = ComputeCost(X_val, Y_val, W, b, lamb)\n",
    "        acc_hist_val.append(acc), cost_hist_val.append(cost), loss_hist_val.append(loss)\n",
    "        print(\"Epoch:\", epoch, \"Accuracy:\", acc_hist[-1])\n",
    "        \n",
    "    return W, b, cost_hist, acc_hist, loss_hist, cost_hist_val, acc_hist_val, loss_hist_val\n",
    "\n",
    "X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, W, b = init_variables()\n",
    "lamb = 1\n",
    "n_epochs = 2\n",
    "n_batch = 100\n",
    "eta = 0.001\n",
    "W, b, cost_hist, acc_hist, loss_hist, cost_hist_val, acc_hist_val, loss_hist_val = MiniBatchGD(X=X_train, Y=Y_train, y=y_train, W=W, b=b, lamb=lamb, n_epochs=n_epochs, n_batch=n_batch, eta=eta, X_val=X_val, Y_val=Y_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(n_epochs+1)]\n",
    "plt.clf()\n",
    "plt.title(\"Cost graph\")\n",
    "plt.plot(x, cost_hist, label = \"Training\")\n",
    "plt.plot(x, cost_hist_val, label = \"Valuation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.title(\"Loss graph\")\n",
    "plt.plot(x, loss_hist, label = \"Training\")\n",
    "plt.plot(x, loss_hist_val, label = \"Valuation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.title(\"Accuracy graph\")\n",
    "plt.plot(x, acc_hist, label = \"Training\")\n",
    "plt.plot(x, acc_hist_val, label = \"Valuation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final test accuracy:\", ComputeAccuracy(X_test, y_test, W, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "montage(W)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d347fc5a6955233192a09f9397fa6c8b692f23ad9cb0e71e223632c12da0ca43"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
