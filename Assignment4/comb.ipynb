{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1656,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1657,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL MACHINE LEARNING FUNCTIONS\n",
    "def softmax(s):\n",
    "    return np.exp(s) / np.sum(np.exp(s), axis=0)\n",
    "\n",
    "def compute_loss(y, p):\n",
    "\n",
    "    return -np.sum(np.log(np.sum(y * p, axis=0)))\n",
    "\n",
    "def adagrad(m_old, g, param_old, eta):\n",
    "    m = m_old + np.power(g, 2)\n",
    "    param = param_old - (eta / np.sqrt(m + np.finfo('float').eps)) * g\n",
    "\n",
    "    return param, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1658,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "def read():\n",
    "  with open(\"goblet_book.txt\") as file:\n",
    "      text = file.read()\n",
    "  return text\n",
    "\n",
    "def one_hot(vec, conversor):\n",
    "    mat = np.zeros((len(conversor), len(vec)))\n",
    "    for i in range(len(vec)):\n",
    "        mat[conversor[vec[i]], i] = 1\n",
    "\n",
    "    return mat\n",
    "  \n",
    "def synthesize(rnn, h_0, x_0, n):\n",
    "    print(h_0.shape)\n",
    "    print(x_0.shape)\n",
    "    x = np.copy(x_0)\n",
    "    h = np.copy(h_0)[:, np.newaxis]\n",
    "    samples = np.zeros((x_0.shape[0], n))\n",
    "    for t in range(n):\n",
    "        a = rnn.w @ h + rnn.u @ x + rnn.b\n",
    "        h = np.tanh(a)\n",
    "        o = rnn.v @ h + rnn.c\n",
    "        p = softmax(o)\n",
    "        choice = np.random.choice(range(x.shape[0]), 1, p=p.flatten())  # Select random character\n",
    "        # according to probabilities\n",
    "        x = np.zeros(x.shape)\n",
    "        x[choice] = 1\n",
    "        samples[:, t] = x.flatten()\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1659,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN SPECIFIC FUNCTIONS\n",
    "def forward(rnn, h_0, x):\n",
    "    h = np.zeros((h_0.shape[0], x.shape[1]))\n",
    "    a = np.zeros((h_0.shape[0], x.shape[1]))\n",
    "    prob = np.zeros(x.shape)\n",
    "    \n",
    "    for t in range(x.shape[1]):\n",
    "        if t == 0:\n",
    "            a[:, t] = (rnn.w @ h_0[:, np.newaxis] + rnn.u @ x[:, t][:, np.newaxis] + rnn.b).flatten()\n",
    "        else:\n",
    "            a[:, t] = (rnn.w @ h[:, t - 1][:, np.newaxis] + rnn.u @ x[:, t][:, np.newaxis] + rnn.b).flatten()\n",
    "            \n",
    "        h[:, t] = np.tanh(a[:, t])\n",
    "        \n",
    "        o = rnn.v @ h[:, t][:, np.newaxis] + rnn.c\n",
    "        p = softmax(o)\n",
    "        prob[:, t] = p.flatten()\n",
    "\n",
    "    return prob, h, a\n",
    "\n",
    "def backprop(rnn, y, p, h, h_prev, a, x):\n",
    "    grad_h = list()\n",
    "    grad_a = list()\n",
    "    # Computation of the last gradient of o\n",
    "    grad_o = -(y - p).T\n",
    "    # Computation of the last gradients of h and a\n",
    "    grad_h.append(grad_o[-1][np.newaxis, :] @ rnn.v)\n",
    "    grad_a.append((grad_h[-1] @ np.diag(1 - np.power(np.tanh(a[:, -1]), 2))))\n",
    "    # Computation of the remaining gradients of o, h, and a\n",
    "    for t in reversed(range(y.shape[1] - 1)):\n",
    "        grad_h.append(grad_o[t][np.newaxis, :] @ rnn.v + grad_a[-1] @ rnn.w)\n",
    "        grad_a.append(grad_h[-1] @ np.diag(1 - np.power(np.tanh(a[:, t]), 2)))\n",
    "\n",
    "    grad_a.reverse()  # Reverse a gradient so it goes forwards\n",
    "    grad_a = np.vstack(grad_a)  # Stack gradients of a as a matrix\n",
    "    rnn_grads = copy.deepcopy(rnn)  # Define rnn object to store the gradients\n",
    "    rnn_grads.v = grad_o.T @ h.T\n",
    "    h_aux = np.zeros(h.shape)  # Auxiliar h matrix that includes h_prev\n",
    "    h_aux[:, 0] = h_prev\n",
    "    h_aux[:, 1:] = h[:, 0:-1]\n",
    "    \n",
    "    rnn_grads.w = grad_a.T @ h_aux.T\n",
    "    rnn_grads.u = grad_a.T @ x.T\n",
    "    rnn_grads.b = np.sum(grad_a, axis=0)[:, np.newaxis]\n",
    "    rnn_grads.c = np.sum(grad_o, axis=0)[:, np.newaxis]\n",
    "    \n",
    "    return rnn_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENTS\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def computeGrads(rnn, x, y, h, grads):\n",
    "    res_rnn = copy.deepcopy(rnn)\n",
    "    h_prev = np.zeros(rnn.m)\n",
    "    for idx, att in enumerate(['b', 'c', 'u', 'w', 'v']):\n",
    "        grad = np.zeros(getattr(rnn, att).shape)\n",
    "        for i in range(grad.shape[0]):\n",
    "            for j in range(grad.shape[1]):\n",
    "                rnn_try = copy.deepcopy(rnn)\n",
    "                aux = np.copy(getattr(rnn_try, att))\n",
    "                aux[i, j] -= h\n",
    "                setattr(rnn_try, att, aux)\n",
    "                p = forward(rnn_try, h_prev, x)[0]\n",
    "                l1 = compute_loss(y, p)\n",
    "                rnn_try = copy.deepcopy(rnn)\n",
    "                aux = np.copy(getattr(rnn_try, att))\n",
    "                aux[i, j] += h\n",
    "                setattr(rnn_try, att, aux)\n",
    "                p = forward(rnn_try, h_prev, x)[0]\n",
    "                l2 = compute_loss(y, p)\n",
    "                grad[i, j] = (l2 - l1) / (2 * h)\n",
    "        setattr(res_rnn, att, grad)\n",
    "\n",
    "    # print('b')\n",
    "    # print(grads.b)\n",
    "    # print('c')\n",
    "    # print(grads.c)\n",
    "    print('b', np.mean(abs(grads.b - res_rnn.b)))\n",
    "    print('c', np.mean(abs(grads.c - res_rnn.c)))\n",
    "    print('u', np.mean(abs(grads.u - res_rnn.u)))\n",
    "    print('v', np.mean(abs(grads.v - res_rnn.v)))\n",
    "    print('w', np.mean(abs(grads.w - res_rnn.w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1661,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    text = read()\n",
    "    # text = text.lower()\n",
    "    # text = text.replace(\"\\n\", \" \")\n",
    "    indexToToken = {}\n",
    "    n = 0\n",
    "    for letter in text:\n",
    "        if letter not in indexToToken.values():\n",
    "          indexToToken[n] = letter\n",
    "          n+=1\n",
    "\n",
    "    tokenToIndex = {v: k for k, v in indexToToken.items()}\n",
    "\n",
    "    m = 5\n",
    "    eta = 0.1\n",
    "    seq_length = 25\n",
    "    K = len(indexToToken)\n",
    "    sig = 0.01\n",
    "\n",
    "    b = np.zeros((m, 1))\n",
    "    c = np.zeros((K, 1))\n",
    "    U = np.random.rand(m, K)*sig\n",
    "    W = np.random.rand(m, m)*sig\n",
    "    V = np.random.rand(K, m)*sig\n",
    "\n",
    "    return indexToToken, tokenToIndex, m, eta, seq_length, K, b, c, U, W, V, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1662,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradients:\n",
    "  def __init__(self, b, c, U, W, V):\n",
    "    self.b = b\n",
    "    self.c = c\n",
    "    self.U = U\n",
    "    self.W = W\n",
    "    self.V = V\n",
    "\n",
    "\n",
    "def charToVec(index, k):\n",
    "  vec = np.zeros(k)\n",
    "  vec[index] = 1\n",
    "  return vec\n",
    "\n",
    "def charSeqToVecSeq(charSeq, tokenToIndex, K):\n",
    "  veqSeq = np.zeros((K, len(charSeq)))\n",
    "  for col, char in enumerate(charSeq):\n",
    "    row = tokenToIndex.get(char.lower())\n",
    "    veqSeq[row,col] = 1\n",
    "  return veqSeq\n",
    "\n",
    "def softMax(o_t):\n",
    "  \"\"\" Standard definition of the softmax function \"\"\"\n",
    "  return np.exp(o_t) / np.sum(np.exp(o_t), axis=0)\n",
    "\n",
    "def tanh(a_t):\n",
    "  return np.tanh(a_t) # This was an in build function? Really? So much time wasted.\n",
    "\n",
    "class RNN():\n",
    "  def __init__(self):\n",
    "    self.indexToToken, self.tokenToIndex, self.m, self.eta, self.seq_length, self.k, self.b, self.c, self.u, self.w, self.v, self.chars = initialize()\n",
    "    self.h_history, self.a_history, self.o_history, self.p_history, self.loss_history, self.y_history, h_prev_0 = [], [], [], [], [], [], []\n",
    "    self.smoothLossValue = None\n",
    "    \n",
    "  def getFullStop(self):\n",
    "    return charToVec(self.tokenToIndex.get(\".\"), self.k)\n",
    "  \n",
    "  def charSeqToVecSeq(self, charSeq):\n",
    "    return charSeqToVecSeq(charSeq, self.tokenToIndex, self.k)\n",
    "  \n",
    "  \n",
    "  def forward(self, X, h0, Y):\n",
    "    if h0 is None:\n",
    "      h0 = np.zeros((self.m))\n",
    "    h_prev = h0\n",
    "    \n",
    "    self.h_prev_0 = h_prev\n",
    "    h_history = []\n",
    "    a_history = []\n",
    "    p_history = []\n",
    "    y_history = []\n",
    "\n",
    "    # Korrekt hittils\n",
    "    # Transpose X to step through the columns\n",
    "    index = 0\n",
    "    for x_t in X.T:\n",
    "      y_t = Y.T[index]\n",
    "      a_t = np.dot(self.w, h_prev[:, np.newaxis]) + np.dot(self.u, x_t[:, np.newaxis]) + self.b # Yay\n",
    "      h_t = tanh(a_t) # Yay\n",
    "      \n",
    "      o_t = np.dot(self.v, h_t) + self.c # Yay (?)\n",
    "      p_t = softMax(o_t) # Yay\n",
    "      # loss = self.calcLoss(y_t, p_t) # Todo: save loss history\n",
    "\n",
    "      h_history.append(h_t.flatten())\n",
    "      a_history.append(a_t.flatten())\n",
    "      p_history.append(p_t.flatten())\n",
    "      y_history.append(y_t.flatten())\n",
    "      \n",
    "      h_prev = h_t.flatten()\n",
    "      index += 1\n",
    "    \n",
    "    self.h_history, self.a_history, self.p_history, self.y_history = h_history, a_history, p_history, y_history # Overwrite and save for backpropagation\n",
    "    # self.loss_history.append(self.smoothLoss(loss))\n",
    "    \n",
    "    return np.array(p_history).T, np.array(h_history).T, np.array(a_history).T #, loss\n",
    "  \n",
    "  def calcLoss(self, y_t, p_t): # (BAD)! This is not the loss function\n",
    "    # Calculate the cross entropy loss\n",
    "    loss = -np.sum(np.dot(y_t, np.log(p_t)))\n",
    "    return loss\n",
    "  \n",
    "  def gethHistory(self):\n",
    "    h_history_h0 = self.h_history.copy()\n",
    "    for index, h in enumerate(self.h_history):\n",
    "      if index == 0:\n",
    "        h_history_h0[index] = self.h_prev_0\n",
    "      else:\n",
    "        h_history_h0[index] = self.h_history[index-1]\n",
    "    \n",
    "    return h_history_h0\n",
    "  \n",
    "  def backprop(self, y, p, h, h_prev, a, x):\n",
    "    gradW, gradV, gradU = np.zeros(self.w.shape), np.zeros(self.v.shape), np.zeros(self.u.shape)\n",
    "    gradb, gradc = np.zeros(self.b.shape), np.zeros(self.c.shape)\n",
    "\n",
    "    # Claculate all dLdo's\n",
    "    dlDos = np.zeros((self.seq_length, self.k))\n",
    "    dlDas = np.zeros((self.seq_length, self.m))\n",
    "    dlDhs = np.zeros((self.seq_length, self.m))\n",
    "    index = 0\n",
    "    for y_t, p_t in zip(self.y_history, self.p_history):\n",
    "      dLdo_t = -(y_t - p_t) # Yay\n",
    "      dlDos[index] = dLdo_t\n",
    "      index += 1\n",
    "      \n",
    "    # Init dLda t+1\n",
    "    dLdh_t = np.dot(dlDos[-1][np.newaxis, :],self.v) # Last dLdo\n",
    "    dLda_t1 = (dLdh_t.flatten() @ np.diag(1 - np.power(tanh(self.a_history[-1]), 2))) # The last dLDa\n",
    "    \n",
    "    dLdh_t2 = dlDos[-1][np.newaxis, :] @ self.v # !\n",
    "    dLda_t2 = (dLdh_t2[-1] @ np.diag(1 - np.power(np.tanh(a[:, -1]), 2))) # !\n",
    "    \n",
    "    dlDas[-1] = dLda_t1\n",
    "    index = len(self.y_history) - 2\n",
    "    for _ in range(len(self.y_history)-1):\n",
    "      dLdo_t = dlDos[index][np.newaxis, :]\n",
    "      dLdh_t = np.dot(dLdo_t, self.v) + np.dot(dLda_t1, self.w) # Yay?\n",
    "      dLda_t = dLdh_t.flatten() @ np.diag(1 - np.power(tanh(self.a_history[index]), 2))\n",
    "      dlDas[index] = dLda_t\n",
    "      index -= 1\n",
    "      dLda_t1 = dLda_t\n",
    "\n",
    "    gradW, gradV, gradU, gradb, gradc = np.zeros(self.w.shape), np.zeros(self.v.shape), np.zeros(self.u.shape), np.zeros(self.b.shape), np.zeros(self.c.shape)\n",
    "    \n",
    "    h_history_h0 = self.gethHistory() \n",
    "    gradU = np.dot(dlDas.T, x.T)\n",
    "    gradW = np.dot(dlDas.T, h_history_h0)\n",
    "    for t in range(len(self.y_history)):\n",
    "      gradV += np.dot(dlDos[t].reshape(dlDos[t].shape[0], 1), self.h_history[t].reshape(1, self.h_history[t].shape[0]))\n",
    "      gradb += dlDas[t][:,np.newaxis]\n",
    "      gradc += dlDos[t][:,np.newaxis]\n",
    "    \n",
    "    #### !!!\n",
    "    rnn_grads = copy.deepcopy(self)  # Define rnn object to store the gradients\n",
    "    rnn_grads.v = gradV #dlDos.T @ h.T   \n",
    "    rnn_grads.w = gradW  # h_history contains the h0 starting state\n",
    "    rnn_grads.u = gradU\n",
    "    rnn_grads.b = gradb \n",
    "    rnn_grads.c = gradc\n",
    "    #### !!!\n",
    "    return rnn_grads\n",
    "    return Gradients(np.clip(gradb, -5, 5), np.clip(gradc, -5, 5), np.clip(gradU, -5, 5), np.clip(gradW, -5, 5), np.clip(gradV, -5, 5))  \n",
    "  \n",
    "  def backprop2(self, x_history):\n",
    "    gradW, gradV, gradU = np.zeros(self.W.shape), np.zeros(self.V.shape), np.zeros(self.U.shape)\n",
    "    gradb, gradc = np.zeros(self.b.shape), np.zeros(self.c.shape)\n",
    "\n",
    "    # Claculate all dLdo's\n",
    "    dlDos = np.zeros((self.seq_length, self.K))\n",
    "    dlDas = np.zeros((self.seq_length, self.m))\n",
    "    index = 0\n",
    "    for y_t, p_t in zip(self.y_history, self.p_history):\n",
    "      dLdo_t = -(y_t - p_t) # Yay\n",
    "      dlDos[index] = dLdo_t\n",
    "      index += 1\n",
    "      \n",
    "    # Init dLda t+1\n",
    "    dLdh_t = np.dot(self.V.T, dlDos[-1]) # Last dLdo\n",
    "    dLda_t1 = dLdh_t @ np.diag(1 - np.power(tanh(self.a_history[-1]), 2)) # The last dLDa\n",
    "\n",
    "    index = len(self.y_history) - 1\n",
    "    for _ in range(len(self.y_history)):\n",
    "      dLdo_t = dlDos[index]\n",
    "      dLdh_t = np.dot(self.V.T, dLdo_t) + np.dot(self.W.T, dLda_t1) # Yay?\n",
    "      dLda_t = dLdh_t @ np.diag(1 - np.power(tanh(self.a_history[index]), 2))\n",
    "      dlDas[index] = dLda_t\n",
    "      index -= 1\n",
    "      dLda_t1 = dLda_t\n",
    "    \n",
    "    # Compute the gradient loss with respect to W, V, U, b and c\n",
    "    gradW, gradV, gradU, gradb, gradc = np.zeros(self.W.shape), np.zeros(self.V.shape), np.zeros(self.U.shape), np.zeros(self.b.shape), np.zeros(self.c.shape)\n",
    "    \n",
    "    h_history_h0 = self.h_history.copy()\n",
    "    for index, h in enumerate(self.h_history):\n",
    "      h_history_h0[index] = h_history_h0[index-1] if index > 0 else self.h_prev_0 \n",
    "    \n",
    "    for t in range(len(self.y_history)):\n",
    "      gradV += np.dot(dlDos[t].reshape(dlDos[t].shape[0], 1), self.h_history[t].reshape(1, self.h_history[t].shape[0])) # Yay?\n",
    "      gradW += np.dot(dlDas[t], h_history_h0[t]) # h_history contains the h0 starting state\n",
    "      # gradU += np.dot(dlDas.T[t], x_history[t]) # Yay ?\n",
    "      gradb += dlDas[t] # Yay ?\n",
    "      gradc += dlDos[t] # Yay ?\n",
    "    print(gradW)\n",
    "    return Gradients(np.clip(gradb, -5, 5), np.clip(gradc, -5, 5), np.clip(gradU, -5, 5), np.clip(gradW, -5, 5), np.clip(gradV, -5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1663,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b 7.19677836422128e-11\n",
      "c 4.845387812532731e-10\n",
      "u 6.973242597294141e-12\n",
      "v 4.611576485133264e-11\n",
      "w 3.7468503537709064e-11\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'getFullStop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1663], line 86\u001b[0m\n\u001b[0;32m     81\u001b[0m       Y[i] \u001b[39m=\u001b[39m x_prev\n\u001b[0;32m     83\u001b[0m     \u001b[39mreturn\u001b[39;00m rnn\u001b[39m.\u001b[39mmatrixOfOneHotToChars(Y)\n\u001b[1;32m---> 86\u001b[0m main()\n",
      "Cell \u001b[1;32mIn [1663], line 58\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m---> 58\u001b[0m x_0 \u001b[39m=\u001b[39m best_rnn\u001b[39m.\u001b[39;49mgetFullStop()\n\u001b[0;32m     59\u001b[0m \u001b[39m# x_0 = one_hot('.', char_to_ind)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLowest loss: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(best_loss))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'getFullStop'"
     ]
    }
   ],
   "source": [
    "# MAIN ALGORITHM\n",
    "def main():\n",
    "    np.random.seed(101)\n",
    "    rnn = RNN()  # Initialize recurrent neural network object, m=m, eta=eta, seq_length=seq_length, sig=0.01\n",
    "    e = 0  # Pointer in the book\n",
    "    h_prev = np.zeros(rnn.m)  # First hidden state\n",
    "    loss_list = list()\n",
    "    smooth_loss = 0\n",
    "    m_list = [0, 0, 0, 0, 0]\n",
    "    best_rnn = None\n",
    "    best_loss = float('inf')\n",
    "    stop = False\n",
    "    \n",
    "    nIter = int(len(rnn.chars)/25)\n",
    "    for i in range(nIter):\n",
    "        x_chars = rnn.chars[i*25:(i+1)*25]\n",
    "        y_chars = rnn.chars[i*25+1:(i+1)*25+1]\n",
    "        x = rnn.charSeqToVecSeq(x_chars)\n",
    "        y = rnn.charSeqToVecSeq(y_chars)\n",
    "        \n",
    "        p, h, a = rnn.forward(x, h_prev, y)\n",
    "        rnn_grads = rnn.backprop(y, p, h, h_prev, a, x)\n",
    "        \n",
    "        computeGrads(rnn, x, y, 1e-4, rnn_grads)\n",
    "        stop = True\n",
    "        if stop:\n",
    "            break\n",
    "        # Check exploding gradients\n",
    "        for idx, att in enumerate(['b', 'c', 'u', 'w', 'v']):\n",
    "            grad = getattr(rnn_grads, att)\n",
    "            grad = np.clip(grad, -5, 5)\n",
    "            att_new, m_val = adagrad(m_list[idx], grad, getattr(rnn, att), rnn.eta)\n",
    "            setattr(rnn, att, att_new)\n",
    "            m_list[idx] = m_val\n",
    "        if i == 0:\n",
    "            smooth_loss = compute_loss(y, p)\n",
    "            loss_list.append(smooth_loss)\n",
    "            best_rnn = copy.deepcopy(rnn)  # Select first model as the best one\n",
    "            best_loss = smooth_loss  # Update best loss with the new one\n",
    "        else:\n",
    "            smooth_loss = 0.999 * smooth_loss + 0.001 * compute_loss(y, p)\n",
    "            if smooth_loss < best_loss:  # Check model loss\n",
    "                best_rnn = copy.deepcopy(rnn)  # Update best model\n",
    "                best_loss = smooth_loss  # Update best loss\n",
    "            if i*25 % (rnn.seq_length * 100) == 0:\n",
    "                print(smooth_loss)\n",
    "                loss_list.append(smooth_loss)\n",
    "        h_prev = h[:, -1]  # h_prev updated to the last computed hidden state\n",
    "        e += rnn.seq_length  # Update the pointer\n",
    "    e = 0  # Reset e when there are no enough characters\n",
    "    h_prev = np.zeros(h_prev.shape)  # Reset h_prev to 0\n",
    "\n",
    "    \n",
    "    plt.plot(np.arange(len(loss_list)) * 100, loss_list)\n",
    "    plt.xlabel(\"Update step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    x_0 = best_rnn.getFullStop()\n",
    "    # x_0 = one_hot('.', char_to_ind)\n",
    "    print(\"Lowest loss: \" + str(best_loss))\n",
    "    h_prev = np.zeros(rnn.m)\n",
    "    samples = synthesize(best_rnn, h_prev, x_0, 1000) #synthesize(best_rnn, h_prev, x_0, 1000)\n",
    "    samples = [best_rnn.indexToToken[int(np.argmax(samples[:, n]))] for n in range(samples.shape[1])]\n",
    "    print(\"\\n\")\n",
    "    print(\"\".join(samples))\n",
    "    print(\"\\n\")\n",
    "\n",
    "def synthesize(rnn, h0, x0, n):\n",
    "    h_prev, x_prev = h0, x0\n",
    "    Y = np.zeros((n, rnn.k)) # Matrix of One-hot vectors of length n representing letters \n",
    "    for i in range(n):\n",
    "      a_t = np.dot(rnn.w, h_prev)\n",
    "      a_t += np.dot(rnn.u, x_prev) \n",
    "      a_t += rnn.b\n",
    "      h_t = np.tanh(a_t)\n",
    "      o_t = np.dot(rnn.v, h_t) + rnn.c\n",
    "      p_t = softMax(o_t)\n",
    "\n",
    "      h_prev = h_t\n",
    "      x_prev = rnn.genNextX(p_t)\n",
    "      Y[i] = x_prev\n",
    "\n",
    "    return rnn.matrixOfOneHotToChars(Y)\n",
    "    \n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89627e806f793b932dfe80791ab48950fda4fb8e20d46d5d1c8fbf2fdce875b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
