{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "class Gradients:\n",
    "  def __init__(self, b, c, U, W, V):\n",
    "    self.b = b\n",
    "    self.c = c\n",
    "    self.U = U\n",
    "    self.W = W\n",
    "    self.V = V\n",
    "\n",
    "class RNN():\n",
    "  def __init__(self):\n",
    "    self.indexToToken, self.tokenToIndex, self.m, self.eta, self.seq_length, self.K, self.b, self.c, self.U, self.W, self.V, self.text = initialize()\n",
    "    self.h_history, self.a_history, self.o_history, self.p_history, self.loss_history, self.y_history, h_prev_0 = [], [], [], [], [], [], []\n",
    "    self.smoothLossValue = None\n",
    "    \n",
    "  def forward(self, X, h0, Y):\n",
    "    if h0 is None:\n",
    "      h0 = np.zeros((self.m))\n",
    "    h_prev = h0\n",
    "    \n",
    "    self.h_prev_0 = h_prev\n",
    "    h_history = []\n",
    "    a_history = []\n",
    "    p_history = []\n",
    "    y_history = []\n",
    "\n",
    "    # Korrekt hittils\n",
    "    # Transpose X to step through the columns\n",
    "    index = 0\n",
    "    for x_t in X.T:\n",
    "      y_t = Y.T[index]\n",
    "      a_t = np.dot(self.W, h_prev[:, np.newaxis]) + np.dot(self.U, x_t[:, np.newaxis]) + self.b # Yay\n",
    "      h_t = tanh(a_t) # Yay\n",
    "      o_t = np.dot(self.V, h_t) + self.c # Yay (?)\n",
    "      p_t = softMax(o_t) # Yay\n",
    "      # loss = self.calcLoss(y_t, p_t) # Todo: save loss history\n",
    "\n",
    "      h_history.append(h_t.flatten())\n",
    "      a_history.append(a_t.flatten())\n",
    "      p_history.append(p_t.flatten())\n",
    "      y_history.append(y_t.flatten())\n",
    "      \n",
    "      h_prev = h_t.flatten()\n",
    "      index += 1\n",
    "    \n",
    "    self.h_history, self.a_history, self.p_history, self.y_history = h_history, a_history, p_history, y_history # Overwrite and save for backpropagation\n",
    "    loss = self.calcLoss(y_history, p_history)\n",
    "    if self.smoothLossValue == None:\n",
    "      self.smoothLossValue = loss\n",
    "    self.loss_history.append(self.smoothLoss(loss))\n",
    "  \n",
    "    return loss\n",
    "  \n",
    "  def gethHistory(self):\n",
    "    h_history_h0 = self.h_history.copy()\n",
    "    for index, h in enumerate(self.h_history):\n",
    "      if index == 0:\n",
    "        h_history_h0[index] = self.h_prev_0\n",
    "      else:\n",
    "        h_history_h0[index] = self.h_history[index-1]\n",
    "    \n",
    "    return h_history_h0  \n",
    "\n",
    "  # Perform the backward pass on the RNN\n",
    "  def backward(self, x_history):\n",
    "    gradW, gradV, gradU = np.zeros(self.W.shape), np.zeros(self.V.shape), np.zeros(self.U.shape)\n",
    "    gradb, gradc = np.zeros(self.b.shape), np.zeros(self.c.shape)\n",
    "\n",
    "    # Claculate all dLdo's\n",
    "    dlDos = np.zeros((self.seq_length, self.K))\n",
    "    dlDas = np.zeros((self.seq_length, self.m))\n",
    "    dlDhs = np.zeros((self.seq_length, self.m))\n",
    "    index = 0\n",
    "    for y_t, p_t in zip(self.y_history, self.p_history):\n",
    "      dLdo_t = -(y_t - p_t) # Yay\n",
    "      dlDos[index] = dLdo_t\n",
    "      index += 1\n",
    "      \n",
    "    # Init dLda t+1\n",
    "    dLdh_t = np.dot(dlDos[-1][np.newaxis, :],self.V) # Last dLdo\n",
    "    dLda_t1 = (dLdh_t.flatten() @ np.diag(1 - np.power(tanh(self.a_history[-1]), 2))) # The last dLDa\n",
    "    \n",
    "    dlDas[-1] = dLda_t1\n",
    "    index = len(self.y_history) - 2\n",
    "    for _ in range(len(self.y_history)-1):\n",
    "      dLdo_t = dlDos[index][np.newaxis, :]\n",
    "      dLdh_t = np.dot(dLdo_t, self.V) + np.dot(dLda_t1, self.W) \n",
    "      dLda_t = dLdh_t.flatten() @ np.diag(1 - np.power(tanh(self.a_history[index]), 2))\n",
    "      dlDas[index] = dLda_t\n",
    "      index -= 1\n",
    "      dLda_t1 = dLda_t\n",
    "\n",
    "    gradW, gradV, gradU, gradb, gradc = np.zeros(self.W.shape), np.zeros(self.V.shape), np.zeros(self.U.shape), np.zeros(self.b.shape), np.zeros(self.c.shape)\n",
    "    \n",
    "    h_history_h0 = self.gethHistory() \n",
    "    gradU = np.dot(dlDas.T, x_history.T)\n",
    "    gradW = np.dot(dlDas.T, h_history_h0)\n",
    "    for t in range(len(self.y_history)):\n",
    "      gradV += np.dot(dlDos[t].reshape(dlDos[t].shape[0], 1), self.h_history[t].reshape(1, self.h_history[t].shape[0]))\n",
    "      gradb += dlDas[t][:,np.newaxis]\n",
    "      gradc += dlDos[t][:,np.newaxis]\n",
    "    \n",
    "    return Gradients(np.clip(gradb, -5, 5), np.clip(gradc, -5, 5), np.clip(gradU, -5, 5), np.clip(gradW, -5, 5), np.clip(gradV, -5, 5)) \n",
    "\n",
    "  # RNN is a RNN object.\n",
    "  # h0 is an input vector of the hidden state at t=0.\n",
    "  # x0 the dummy state at t=0.\n",
    "  # n is the length of the sequence to be generated.\n",
    "  def synthesize(self, h0, x0, n):\n",
    "    h_prev, x_prev = h0, x0\n",
    "    Y = np.zeros((n, self.K)) # Matrix of One-hot vectors of length n representing letters \n",
    "    for i in range(n): # Do a forward pass and sample from the output distribution\n",
    "      a_t = np.dot(self.W, h_prev[:, np.newaxis]) \n",
    "      a_t += np.dot(self.U, x_prev[:, np.newaxis]) \n",
    "      a_t += self.b\n",
    "      h_t = np.tanh(a_t)\n",
    "      o_t = np.dot(self.V, h_t) + self.c\n",
    "      p_t = softMax(o_t)\n",
    "      h_prev = h_t.flatten()\n",
    "      x_prev = self.genNextX(p_t) # Get a character according to the p_t probability distribution\n",
    "      Y[i] = x_prev\n",
    "\n",
    "    return self.matrixOfOneHotToChars(Y)\n",
    "    \n",
    "  # p is a Kx1 vector\n",
    "  def genNextX(self, p):\n",
    "    rand = random.uniform(0, 1)\n",
    "    cumProb = 0\n",
    "    index = 0\n",
    "    for i, prob in enumerate(p):\n",
    "      cumProb += prob\n",
    "      if cumProb >= rand:\n",
    "        index = i\n",
    "        break\n",
    "    \n",
    "    return charToVec(index, self.K)\n",
    "    \n",
    "  def calcLoss(self, y_t, p_t):\n",
    "    # Calculate the cross entropy loss\n",
    "    y, p = np.asarray(y_t), np.asarray(p_t)\n",
    "    return -np.sum(y * np.log(p)) \n",
    "\n",
    "  def charSeqToVecSeq(self, charSeq):\n",
    "    return charSeqToVecSeq(charSeq, self.tokenToIndex, self.K)\n",
    "\n",
    "  def matrixOfOneHotToChars(self, Y):\n",
    "    outputSequence = \"\"\n",
    "    for K in Y:\n",
    "      charIndex = np.where(K==1)[0][0]\n",
    "      outputSequence += self.indexToToken.get(charIndex)\n",
    "    \n",
    "    return outputSequence\n",
    "\n",
    "  def getFullStop(self):\n",
    "    return charToVec(self.tokenToIndex.get(\".\"), self.K)\n",
    "  \n",
    "  def smoothLoss(self, loss):\n",
    "    toReturn = self.smoothLossValue * 0.999 + loss * 0.001\n",
    "    self.smoothLossValue = toReturn\n",
    "    return toReturn\n",
    "\n",
    "def charToVec(index, K):\n",
    "  vec = np.zeros(K)\n",
    "  vec[index] = 1\n",
    "  return vec\n",
    "\n",
    "def tanh(a_t):\n",
    "  return np.tanh(a_t) # This was an in build function? Really? So much time wasted.\n",
    "\n",
    "def softMax(o_t):\n",
    "  # Standard definition of the softmax function\n",
    "  return np.exp(o_t) / np.sum(np.exp(o_t), axis=0)\n",
    "\n",
    "def read():\n",
    "  with open(\"goblet_book.txt\") as file:\n",
    "      text = file.read()\n",
    "  return text\n",
    "\n",
    "def initialize():\n",
    "    text = read()\n",
    "    # text = text.lower()\n",
    "    # text = text.replace(\"\\n\", \" \")\n",
    "    indexToToken = {}\n",
    "    n = 0\n",
    "    for letter in text:\n",
    "        if letter not in indexToToken.values():\n",
    "          indexToToken[n] = letter\n",
    "          n+=1\n",
    "\n",
    "    tokenToIndex = {v: k for k, v in indexToToken.items()}\n",
    "\n",
    "    m = 100\n",
    "    eta = 0.1\n",
    "    seq_length = 25\n",
    "    K = len(indexToToken)\n",
    "    sig = 0.01\n",
    "\n",
    "    b = np.zeros((m, 1))\n",
    "    c = np.zeros((K, 1))\n",
    "    U = np.random.rand(m, K)*sig\n",
    "    W = np.random.rand(m, m)*sig\n",
    "    V = np.random.rand(K, m)*sig\n",
    "\n",
    "    return indexToToken, tokenToIndex, m, eta, seq_length, K, b, c, U, W, V, text\n",
    "\n",
    "def charSeqToVecSeq(charSeq, tokenToIndex, K):\n",
    "  veqSeq = np.zeros((K, len(charSeq)))\n",
    "  for col, char in enumerate(charSeq):\n",
    "    row = tokenToIndex.get(char) # .lower()\n",
    "    veqSeq[row,col] = 1\n",
    "  return veqSeq\n",
    "\n",
    "def adaGrad(m, gradient, theRnn, param):\n",
    "  m += np.square(gradient)\n",
    "  param -= np.divide(theRnn.eta, np.sqrt(m) + EPSILON) * gradient # epsilon does not have to be included in the sqrt\n",
    "  return m, param\n",
    "    \n",
    "def trainRNN():\n",
    "  rnnH = RNN() # The harry potter rnn :)\n",
    "  x0 = rnnH.getFullStop() # a \".\"-character (period, fullstop)\n",
    "  text = rnnH.text\n",
    "  mValues = [np.zeros(rnnH.b.shape), np.zeros(rnnH.c.shape), np.zeros(rnnH.U.shape), np.zeros(rnnH.W.shape), np.zeros(rnnH.V.shape)]\n",
    "  nIter = int(len(text)/25) # Step through the text in chunks of 25 characters\n",
    "  epochs = 5\n",
    "  bestRNN = None\n",
    "  bestLoss = float(\"inf\")\n",
    "  for epoch in range(epochs):\n",
    "    h0 = None # The initial hidden state is set to zero. (see forward pass implmentation)\n",
    "    for i in range(nIter): # ignore the last characters that don't fit into a sequence of length 25\n",
    "      x_chars = text[i*25:(i+1)*25]\n",
    "      y_chars = text[i*25+1:(i+1)*25+1]\n",
    "      X_char_vecs = charSeqToVecSeq(x_chars, rnnH.tokenToIndex, rnnH.K)\n",
    "      Y_char_vecs = charSeqToVecSeq(y_chars, rnnH.tokenToIndex, rnnH.K)\n",
    "      \n",
    "      if epoch == 0 and i == 0:\n",
    "        print('Not trained: ', rnnH.synthesize(np.zeros((rnnH.m)), x0, 200))\n",
    "      \n",
    "      loss = rnnH.forward(X_char_vecs, h0, Y_char_vecs)\n",
    "      gradients = rnnH.backward(X_char_vecs)\n",
    "      \n",
    "      h0 = rnnH.h_history[-1] # The hidden state of the last time step is used as the initial hidden state for the next sequence.\n",
    "      \n",
    "      # Update the parameters using AdaGrad\n",
    "      mValues[0], rnnH.b = adaGrad(mValues[0], gradients.b, rnnH, rnnH.b)\n",
    "      mValues[1], rnnH.c = adaGrad(mValues[1], gradients.c, rnnH, rnnH.c)\n",
    "      mValues[2], rnnH.U = adaGrad(mValues[2], gradients.U, rnnH, rnnH.U)\n",
    "      mValues[3], rnnH.W = adaGrad(mValues[3], gradients.W, rnnH, rnnH.W)\n",
    "      mValues[4], rnnH.V = adaGrad(mValues[4], gradients.V, rnnH, rnnH.V)\n",
    "      \n",
    "      if loss < bestLoss:\n",
    "        bestLoss = loss\n",
    "        bestRNN = copy.deepcopy(rnnH)\n",
    "      \n",
    "      if ((i + epoch * nIter) % 10000 == 0):\n",
    "        generatedText = rnnH.synthesize(h0, x0, 200)\n",
    "        funWords = ['Harry', 'Hermione', 'Weasly', 'Dumbledore', 'Voldemort']\n",
    "        for word in funWords:\n",
    "          if word in generatedText:\n",
    "            print(f'\"{word}\" is in the generated text!\\n{generatedText}\\n')\n",
    "        print(f'Text: (loss: {rnnH.loss_history[-1]}, iteration no. {i + epoch * nIter}, {(1-((i + epoch * nIter)/(5*nIter)))*100}% left)\\n{generatedText}\\n')\n",
    "  \n",
    "  print('Best loss: ' + str(bestLoss))\n",
    "  return rnnH, bestRNN, x0, None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop and debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnH, bestRNN, x0, h0 = trainRNN()\n",
    "\n",
    "x = [i for i in range(len(rnnH.loss_history))]\n",
    "plt.plot(x, rnnH.loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Smooth loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestRNN.synthesize(bestRNN.h_history[-1], x0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GradientFunctions\n",
    "\n",
    "# def debugRNN():\n",
    "#     rnn = RNN()\n",
    "#     text = rnn.text\n",
    "#     X_chars = text[0:25]\n",
    "#     Y_chars = text[1:26]\n",
    "#     X_char_vecs = charSeqToVecSeq(X_chars, rnn.tokenToIndex, rnn.K)\n",
    "#     Y_char_vecs = charSeqToVecSeq(Y_chars, rnn.tokenToIndex, rnn.K)\n",
    "\n",
    "#     rnn.forward(X_char_vecs, None, Y_char_vecs)\n",
    "#     grad = rnn.backward(X_char_vecs)\n",
    "\n",
    "#     rnn.forward(X_char_vecs, None, Y_char_vecs)\n",
    "#     grad = rnn.backward(X_char_vecs)\n",
    "#     numgrads = GradientFunctions.numericalGradient(rnn, X_char_vecs, Y_char_vecs, 1e-6, grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89627e806f793b932dfe80791ab48950fda4fb8e20d46d5d1c8fbf2fdce875b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
