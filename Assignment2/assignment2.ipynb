{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def LoadBatch(file): \n",
    "    \n",
    "    with open(\"data/\"+file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    \n",
    "    pixelDat = dict[b'data']\n",
    "    labels = dict[b'labels']\n",
    "    labelsOneHot = np.zeros((len(labels),10))\n",
    "    \n",
    "    for index in range(len(labels)): # Not efficient :)\n",
    "        labelsOneHot[index][labels[index]] = 1\n",
    "        \n",
    "    return pixelDat, labelsOneHot, labels\n",
    "\n",
    "def CalcS(X, W, b):\n",
    "    return W @ X + b\n",
    "\n",
    "def EvaluateClassifier(X, W1, W2, b1, b2): # Returns the final P values and the intermidiary activation values (H)\n",
    "    s1 = CalcS(X, W1, b1) # s1 is mxD, TODO: Check if we should diagnoalize b1\n",
    "    H = np.maximum(0, s1) # H is mxD, , returns the element-wise max of s1 and 0\n",
    "    s2 = CalcS(H, W2, b2) # s2 is mxC\n",
    "    P = softmax(s2)\n",
    "    return P, H\n",
    "\n",
    "def ComputeCost(X, Y, W1, W2, b1, b2, lamb): \n",
    "    J = 0\n",
    "    P,_ = EvaluateClassifier(X, W1, W2, b1, b2)\n",
    "    P_t = np.clip(P.T, 1e-15, None)\n",
    "    for i in range(len(P_t)):\n",
    "        J += -np.dot(Y[i], np.log(P_t[i], where=P_t[i] > 0))\n",
    "    J /= len(X[0]) # Divide by dimensionality\n",
    "    loss = J # For documentation\n",
    "    J += lamb * (np.sum(np.power(W1,2)) + np.sum(np.power(W2,2))) # WTerm\n",
    "    \n",
    "    return J, P, loss\n",
    "\n",
    "def ComputeAccuracy(X, y, W1, W2, b1, b2): \n",
    "    nCorr = 0\n",
    "    P,_ = EvaluateClassifier(X, W1, W2, b1, b2)\n",
    "    for index in range(X.T.shape[0]):\n",
    "        p = P.T[index]\n",
    "        predClass = np.argmax(p)\n",
    "        if predClass == y[index]:\n",
    "            nCorr += 1\n",
    "    \n",
    "    acc = nCorr/X.T.shape[0]\n",
    "    return acc\n",
    "\n",
    "def ComputeGradients(X, Y, P, H, W1, W2, lamb, b_start=0, b_size=20): # TODO: Convert \n",
    "    X_batch, Y_batch, P_batch, H_batch = X.T[b_start:b_start+b_size].T, Y[b_start:b_start+b_size].T, P.T[b_start:b_start+b_size].T, H.T[b_start:b_start+b_size].T # Because Python\n",
    "    G_vec = - (Y_batch-P_batch) \n",
    "    # print(\"weights[-1]:\", W2.shape)\n",
    "    # print(\"data[-1]:\", H_batch.shape)\n",
    "    # print(\"weights[-2]:\", W1.shape)\n",
    "    # print(\"data[-2]:\", X_batch.shape)\n",
    "    # print(G_vec.shape)\n",
    "    dJdW2 = (G_vec @ H_batch.T)/b_size \n",
    "    dJdb2 = np.sum(G_vec, axis=1)[:, np.newaxis]/b_size # error in notes?, check [:, np.newaxis]\n",
    "    \n",
    "    G_vec_2 = W2.T @ G_vec \n",
    "    H_batch[H_batch<0] = 0\n",
    "    H_batch[H_batch>0] = 1\n",
    "\n",
    "    G_vec_2 = np.multiply(G_vec_2, H_batch)\n",
    "\n",
    "    dJdW1 = G_vec_2 @ X_batch.T / b_size # grad_W1 is MxD\n",
    "    dJdb1 = np.sum(G_vec_2, axis=1)[:, np.newaxis]/b_size\n",
    "    grad_W1 = dJdW1 + 2*lamb*W1\n",
    "    grad_b1 = dJdb1 \n",
    "    grad_W2 = dJdW2 + 2*lamb*W2 \n",
    "    grad_b2 = dJdb2 \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "\n",
    "def init_variables2(): # More training data\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = None, None, None, None, None, None\n",
    "    y_train = None\n",
    "    for file in [\"data_batch_1\", \"data_batch_2\", \"data_batch_3\", \"data_batch_4\", \"data_batch_5\", \"test_batch\"]:\n",
    "        X, Y, y = LoadBatch(file)\n",
    "        mean_X = np.mean(X, axis=0) \n",
    "        std_X = np.std(X, axis=0)\n",
    "        X = X - mean_X\n",
    "        X = X / std_X\n",
    "        X = X.T # Make x stored in columns\n",
    "        if file in [\"data_batch_1\",\"data_batch_3\", \"data_batch_4\", \"data_batch_5\"]:\n",
    "            if X_train is None:\n",
    "                X_train = X\n",
    "                Y_train = Y\n",
    "                y_train = y\n",
    "            else:\n",
    "                X_train = np.concatenate((X_train, X), axis=1)\n",
    "                Y_train = np.concatenate((Y_train, Y), axis=0)\n",
    "                y_train += y\n",
    "            \n",
    "        elif file == \"data_batch_2\":\n",
    "            X_val,Y_val, y_val = X, Y, y\n",
    "        else:\n",
    "            X_test,Y_test, y_test = X, Y, y\n",
    "    \n",
    "    np.random.seed(111)\n",
    "    eta_min = 0.00001\n",
    "    eta_max = 0.1\n",
    "    m = 50 # ?\n",
    "    K = 10 # Number of labels\n",
    "    d = len(X.T[0]) # dimensionality\n",
    "    W1 = np.random.normal(0, 1/np.sqrt(d), (m, d)) \n",
    "    W2 = np.random.normal(0, 1/np.sqrt(m), (K, m)) \n",
    "    b1 = np.zeros((m,1)) # np.random.normal(0, 0.01, (m,1))\n",
    "    b2 = np.zeros((K,1)) # np.random.normal(0, 0.01, (K,1))\n",
    "    n_s=500\n",
    "            \n",
    "    return X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, W1, W2, b1, b2, eta_min, eta_max, n_s\n",
    "\n",
    "def init_variables(): # Excercise 1\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = None, None, None, None, None, None\n",
    "    for file in [\"data_batch_1\", \"data_batch_2\", \"test_batch\"]:\n",
    "        X, Y, y = LoadBatch(file)\n",
    "        mean_X = np.mean(X, axis=0) \n",
    "        std_X = np.std(X, axis=0)\n",
    "        X = X - mean_X\n",
    "        X = X / std_X\n",
    "        X = X.T # Make x stored in columns\n",
    "        if file == \"data_batch_1\":\n",
    "            X_train,Y_train, y_train = X, Y, y\n",
    "        elif file == \"data_batch_2\":\n",
    "            X_val,Y_val, y_val = X, Y, y\n",
    "        else:\n",
    "            X_test,Y_test, y_test = X, Y, y\n",
    "    \n",
    "    np.random.seed(111)\n",
    "    eta_min = 0.00001\n",
    "    eta_max = 0.1\n",
    "    m = 50 # ?\n",
    "    K = 10 # Number of labels\n",
    "    d = len(X.T[0]) # dimensionality\n",
    "    W1 = np.random.normal(0, 1/np.sqrt(d), (m, d)) \n",
    "    W2 = np.random.normal(0, 1/np.sqrt(m), (K, m)) \n",
    "    b1 = np.zeros((m,1)) # np.random.normal(0, 0.01, (m,1))\n",
    "    b2 = np.zeros((K,1)) # np.random.normal(0, 0.01, (K,1))\n",
    "    n_s=500\n",
    "            \n",
    "    return X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, W1, W2, b1, b2, eta_min, eta_max, n_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, W1, W2, b1, b2, eta_min, eta_max,_ = init_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 0\n",
    "S1 = CalcS(X_train, W1, b1)\n",
    "H = np.maximum(0, S1)\n",
    "S2 = CalcS(H, W2, b2)\n",
    "P = softmax(S2)\n",
    "J,_,_ = ComputeCost(X_train, Y_train, W1, W2, b1, b2, lamb)\n",
    "acc = ComputeAccuracy(X_train, y_train, W1, W2, b1, b2)\n",
    "grad_W1, grad_W2, grad_b1, grad_b2 = ComputeGradients(X_train, Y_train, P, H, W1, W2, lamb, b_start=0, b_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_weights,grad_bias = compute_grads_num_2(X_train.T[0:10].T, Y_train.T[0:10].T, [W1, W2], [b1, b2], lamb, h=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_W1, grad_W2, grad_b1, grad_b2 = ComputeGradients(X_train, Y_train, P, H, W1, W2, lamb, b_start=0, b_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(grad_bias[0])):\n",
    "    if grad_bias[0][i]-grad_b1[i] > 1e-7:\n",
    "        print(grad_bias[0][i]-grad_b1[i])\n",
    "        \n",
    "for i in range(len(grad_weights[0])):\n",
    "    for j in range(len(grad_weights[0][i])):\n",
    "        if grad_weights[0][i][j]-(grad_W1[i][j]) > 1e-7:\n",
    "            print(grad_weights[0][i][j]-(grad_W1[i][j]))\n",
    "# diff = np.subtract(grad_b1, grad_bias[0])\n",
    "# diff = np.absolute(diff)\n",
    "\n",
    "# print(diff.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lrhist = []\n",
    "def getLr(t, eta_min, eta_max, n_s):\n",
    "    if (int(t/n_s))%2 == 0:\n",
    "        return eta_min + (eta_max-eta_min)*((t%n_s)/n_s)\n",
    "    else:\n",
    "        return eta_max - (eta_max-eta_min)*((t%n_s)/n_s)\n",
    "    \n",
    "    \n",
    "def MiniBatchGD(X, Y, y, W1, W2, b1, b2, lamb, n_epochs, n_batch, eta_min, eta_max, X_val, Y_val, y_val, n_s):         \n",
    "    acc_hist,cost_hist, loss_hist, acc_hist_val, cost_hist_val, loss_hist_val,loss_hist_val = [] ,[], [], [], [], [], []\n",
    "    # Train, initial val\n",
    "    acc = ComputeAccuracy(X, y, W1, W2, b1, b2)\n",
    "    cost, _, loss = ComputeCost(X, Y, W1, W2, b1, b2, lamb)\n",
    "    acc_hist.append(acc), cost_hist.append(cost), loss_hist.append(loss)\n",
    "    # Validation, initial val\n",
    "    acc = ComputeAccuracy(X_val, y_val, W1, W2, b1, b2)\n",
    "    cost, _, loss = ComputeCost(X_val, Y_val, W1, W2, b1, b2, lamb)\n",
    "    acc_hist_val.append(acc), cost_hist_val.append(cost), loss_hist_val.append(loss)\n",
    "    t = 0\n",
    "    for epoch in tqdm(range(n_epochs)): # Main loop\n",
    "        for batch in range(int(len(Y)/n_batch)):\n",
    "            t+=1\n",
    "            lr = getLr(t, eta_min, eta_max, n_s)\n",
    "            lrhist.append(lr)\n",
    "            P, H = EvaluateClassifier(X, W1, W2, b1, b2)\n",
    "            grad_W1, grad_W2, grad_b1, grad_b2 = ComputeGradients(X, Y, P, H, W1, W2, lamb, b_start=batch*n_batch, b_size=n_batch)\n",
    "            W1 = W1 - grad_W1*lr\n",
    "            W2 = W2 - grad_W2*lr\n",
    "            # grad_b = grad_b.reshape(b.shape)\n",
    "            b1= b1 - grad_b1*lr\n",
    "            b2 = b2 - grad_b2*lr\n",
    "        # Train\n",
    "        acc = ComputeAccuracy(X, y, W1, W2, b1, b2)\n",
    "        cost, _, loss = ComputeCost(X, Y, W1, W2, b1, b2, lamb)\n",
    "        acc_hist.append(acc), cost_hist.append(cost), loss_hist.append(loss)\n",
    "        # Validation\n",
    "        acc = ComputeAccuracy(X_val, y_val, W1, W2, b1, b2)\n",
    "        cost, _, loss = ComputeCost(X_val, Y_val, W1, W2, b1, b2, lamb)\n",
    "        acc_hist_val.append(acc), cost_hist_val.append(cost), loss_hist_val.append(loss)\n",
    "        print(\"Epoch:\", epoch, \"Accuracy:\", acc_hist[-1])\n",
    "        \n",
    "    return W1, W2, b1, b2, cost_hist, acc_hist, loss_hist, cost_hist_val, acc_hist_val, loss_hist_val\n",
    "\n",
    "lamb = 0.01\n",
    "n_epochs = 24 # 1.5 \n",
    "n_batch = 100\n",
    "n_s = 2 * int(len(X_train[0]))\n",
    "eta = 0.001\n",
    "\n",
    "l_min = -4\n",
    "l_max = -3\n",
    "search = [i for i in range(10)]\n",
    "W1hist, W2hist, b1hist, b2hist, cost_hist_list, acc_hist_list, loss_hist_list, cost_hist_list_val, acc_hist_list_val, loss_hist_list_val = [], [], [], [], [], [], [], [], [], []\n",
    "n = 0\n",
    "for i in search:\n",
    "    X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, W1, W2, b1, b2, eta_min, eta_max, n_s = init_variables() \n",
    "    print(str(n*10)+\"%\")\n",
    "    n+=1\n",
    "    l = l_min + (l_max - l_min) * i / len(search)\n",
    "    lamb = 10**l\n",
    "    # 10**-3.8 bra\n",
    "    W1, W2, b1, b2, cost_hist, acc_hist, loss_hist, cost_hist_val, acc_hist_val, loss_hist_val = MiniBatchGD(X=X_train, Y=Y_train, y=y_train, W1=W1,W2=W2, b1=b1, b2=b2, lamb=lamb, n_epochs=n_epochs, n_batch=n_batch, eta_min=eta_min, eta_max=eta_max, X_val=X_val, Y_val=Y_val, y_val=y_val, n_s=n_s)\n",
    "    W1hist.append(W1), W2hist.append(W2), b1hist.append(b1), b2hist.append(b2), cost_hist_list.append(cost_hist), acc_hist_list.append(acc_hist), loss_hist_list.append(loss_hist), cost_hist_list_val.append(cost_hist_val), acc_hist_list_val.append(acc_hist_val), loss_hist_list_val.append(loss_hist_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(len(lrhist))]\n",
    "plt.clf()\n",
    "plt.title(\"Learning rate\")\n",
    "plt.plot(x, lrhist, label = \"Lr\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting\n",
    "x = [i for i in range(n_epochs+1)]\n",
    "\n",
    "for i in range(len(W1hist)):\n",
    "    W1, W2, b1, b2, cost_hist, acc_hist, loss_hist, cost_hist_val, acc_hist_val, loss_hist_val = W1hist[i], W2hist[i], b1hist[i], b2hist[i], cost_hist_list[i], acc_hist_list[i], loss_hist_list[i], cost_hist_list_val[i], acc_hist_list_val[i], loss_hist_list_val[i]\n",
    "    l = l_min + (l_max - l_min) * i / len(search)\n",
    "    lamb = 10**l\n",
    "    print(\"\\nLambda:\", lamb, \"\\n----------------------------------\")\n",
    "    plt.clf()\n",
    "    plt.title(\"Cost graph\")\n",
    "    plt.plot(x, cost_hist, label = \"Training\")\n",
    "    plt.plot(x, cost_hist_val, label = \"Valuation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.title(\"Loss graph\")\n",
    "    plt.plot(x, loss_hist, label = \"Training\")\n",
    "    plt.plot(x, loss_hist_val, label = \"Valuation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.title(\"Accuracy graph\")\n",
    "    plt.plot(x, acc_hist, label = \"Training\")\n",
    "    plt.plot(x, acc_hist_val, label = \"Valuation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"Final test accuracy:\", ComputeAccuracy(X_test, y_test, W1, W2, b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lrhist = []\n",
    "def getLr(t, eta_min, eta_max, n_s):\n",
    "    if (int(t/n_s))%2 == 0:\n",
    "        return eta_min + (eta_max-eta_min)*((t%n_s)/n_s)\n",
    "    else:\n",
    "        return eta_max - (eta_max-eta_min)*((t%n_s)/n_s)\n",
    "    \n",
    "    \n",
    "def MiniBatchGD(X, Y, y, W1, W2, b1, b2, lamb, n_epochs, n_batch, eta_min, eta_max, X_val, Y_val, y_val, n_s):         \n",
    "    acc_hist,cost_hist, loss_hist, acc_hist_val, cost_hist_val, loss_hist_val,loss_hist_val = [] ,[], [], [], [], [], []\n",
    "    # Train, initial val\n",
    "    acc = ComputeAccuracy(X, y, W1, W2, b1, b2)\n",
    "    cost, _, loss = ComputeCost(X, Y, W1, W2, b1, b2, lamb)\n",
    "    acc_hist.append(acc), cost_hist.append(cost), loss_hist.append(loss)\n",
    "    # Validation, initial val\n",
    "    acc = ComputeAccuracy(X_val, y_val, W1, W2, b1, b2)\n",
    "    cost, _, loss = ComputeCost(X_val, Y_val, W1, W2, b1, b2, lamb)\n",
    "    acc_hist_val.append(acc), cost_hist_val.append(cost), loss_hist_val.append(loss)\n",
    "    t = 0\n",
    "    for epoch in tqdm(range(n_epochs)): # Main loop\n",
    "        for batch in range(int(len(Y)/n_batch)):\n",
    "            t+=1\n",
    "            lr = getLr(t, eta_min, eta_max, n_s)\n",
    "            lrhist.append(lr)\n",
    "            P, H = EvaluateClassifier(X, W1, W2, b1, b2)\n",
    "            grad_W1, grad_W2, grad_b1, grad_b2 = ComputeGradients(X, Y, P, H, W1, W2, lamb, b_start=batch*n_batch, b_size=n_batch)\n",
    "            W1 = W1 - grad_W1*lr\n",
    "            W2 = W2 - grad_W2*lr\n",
    "            # grad_b = grad_b.reshape(b.shape)\n",
    "            b1= b1 - grad_b1*lr\n",
    "            b2 = b2 - grad_b2*lr\n",
    "        # Train\n",
    "        acc = ComputeAccuracy(X, y, W1, W2, b1, b2)\n",
    "        cost, _, loss = ComputeCost(X, Y, W1, W2, b1, b2, lamb)\n",
    "        acc_hist.append(acc), cost_hist.append(cost), loss_hist.append(loss)\n",
    "        # Validation\n",
    "        acc = ComputeAccuracy(X_val, y_val, W1, W2, b1, b2)\n",
    "        cost, _, loss = ComputeCost(X_val, Y_val, W1, W2, b1, b2, lamb)\n",
    "        acc_hist_val.append(acc), cost_hist_val.append(cost), loss_hist_val.append(loss)\n",
    "        print(\"Epoch:\", epoch, \"Accuracy:\", acc_hist[-1])\n",
    "        \n",
    "    return W1, W2, b1, b2, cost_hist, acc_hist, loss_hist, cost_hist_val, acc_hist_val, loss_hist_val\n",
    "\n",
    "lamb = 0.01\n",
    "n_epochs = 30 # 1.5 \n",
    "n_batch = 100\n",
    "n_s = 800 #2 * int(len(X_train[0]))\n",
    "eta = 0.001\n",
    "\n",
    "W1hist, W2hist, b1hist, b2hist, cost_hist_list, acc_hist_list, loss_hist_list, cost_hist_list_val, acc_hist_list_val, loss_hist_list_val = [], [], [], [], [], [], [], [], [], []\n",
    "X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, W1, W2, b1, b2, eta_min, eta_max, n_s = init_variables() \n",
    "lamb = 0.01\n",
    "W1, W2, b1, b2, cost_hist, acc_hist, loss_hist, cost_hist_val, acc_hist_val, loss_hist_val = MiniBatchGD(X=X_train, Y=Y_train, y=y_train, W1=W1,W2=W2, b1=b1, b2=b2, lamb=lamb, n_epochs=n_epochs, n_batch=n_batch, eta_min=eta_min, eta_max=eta_max, X_val=X_val, Y_val=Y_val, y_val=y_val, n_s=n_s)\n",
    "W1hist.append(W1), W2hist.append(W2), b1hist.append(b1), b2hist.append(b2), cost_hist_list.append(cost_hist), acc_hist_list.append(acc_hist), loss_hist_list.append(loss_hist), cost_hist_list_val.append(cost_hist_val), acc_hist_list_val.append(acc_hist_val), loss_hist_list_val.append(loss_hist_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting\n",
    "x = [i for i in range(n_epochs+1)]\n",
    "\n",
    "for i in range(len(W1hist)):\n",
    "    W1, W2, b1, b2, cost_hist, acc_hist, loss_hist, cost_hist_val, acc_hist_val, loss_hist_val = W1hist[i], W2hist[i], b1hist[i], b2hist[i], cost_hist_list[i], acc_hist_list[i], loss_hist_list[i], cost_hist_list_val[i], acc_hist_list_val[i], loss_hist_list_val[i]\n",
    "    plt.clf()\n",
    "    plt.title(\"Cost graph\")\n",
    "    plt.plot(x, cost_hist, label = \"Training\")\n",
    "    plt.plot(x, cost_hist_val, label = \"Valuation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.title(\"Loss graph\")\n",
    "    plt.plot(x, loss_hist, label = \"Training\")\n",
    "    plt.plot(x, loss_hist_val, label = \"Valuation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.title(\"Accuracy graph\")\n",
    "    plt.plot(x, acc_hist, label = \"Training\")\n",
    "    plt.plot(x, acc_hist_val, label = \"Valuation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"Final test accuracy:\", ComputeAccuracy(X_test, y_test, W1, W2, b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting\n",
    "x = [i for i in range(len(lrhist))]\n",
    "plt.clf()\n",
    "plt.title(\"Learning rate graph\")\n",
    "plt.plot(x, lrhist, label = \"lr\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f9fdc84931d9f6289092912b77130bd52b85d6d81d05617547b529bfc091984"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
